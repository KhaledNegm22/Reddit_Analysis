{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "En6gkn1WoGrh",
        "outputId": "3730d7c1-87cc-4816-df76-bb637ccc1820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.8.3)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install praw\n",
        "import pandas as pd\n",
        "import praw\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"praw\")\n",
        "\n",
        "logging.getLogger(\"praw\").setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"#######\",\n",
        "    client_secret=\"##########\",\n",
        "    user_agent=\"############\",\n",
        ")\n",
        "\n",
        "MLsubreddit = reddit.subreddit(\"MachineLearning\")\n",
        "\n",
        "\n",
        "##  Post-level: id, title, author, score, num_comments, created_utc, selftext,subreddit\n",
        "## Comment-level: id, author, body, score, created_utc, parent_id, link_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "subs = MLsubreddit.top('year')\n",
        "for submission in subs:\n",
        "    print(submission.title)\n",
        "\n",
        "    print(submission.score)\n",
        "\n",
        "    print(submission.id)\n",
        "\n",
        "    print(submission.url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tkH3-7t1oWmt",
        "outputId": "64159dd0-abdb-4832-fb87-9906a33afdd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2265198772.py:1: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
            "Call this function with 'time_filter' as a keyword argument.\n",
            "  subs = MLsubreddit.top('year')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[D] The \"it\" in AI models is really just the dataset?\n",
            "1309\n",
            "1cjxh9u\n",
            "https://i.redd.it/uadactn53eyc1.png\n",
            "[N] 2024 Nobel Prize for Physics goes to ML and DNN researchers J. Hopfield and G. Hinton\n",
            "1162\n",
            "1fywi9h\n",
            "https://www.reddit.com/r/MachineLearning/comments/1fywi9h/n_2024_nobel_prize_for_physics_goes_to_ml_and_dnn/\n",
            "[D] Why do PhD Students in the US seem like overpowered final bosses \n",
            "1121\n",
            "1g7dzkp\n",
            "https://www.reddit.com/r/MachineLearning/comments/1g7dzkp/d_why_do_phd_students_in_the_us_seem_like/\n",
            "[D]Stuck in AI Hell: What to do in post LLM world\n",
            "839\n",
            "1h7jg87\n",
            "https://www.reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/\n",
            "[D] The winner of the NeurIPS 2024 Best Paper Award  sabotaged the other teams\n",
            "707\n",
            "1hctf36\n",
            "https://www.reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/\n",
            "[D] Can we please stop using \"is all we need\" in titles?\n",
            "702\n",
            "1hlbtrs\n",
            "https://www.reddit.com/r/MachineLearning/comments/1hlbtrs/d_can_we_please_stop_using_is_all_we_need_in/\n",
            "[D] What happened at NeurIPS?\n",
            "634\n",
            "1hdxbru\n",
            "https://i.redd.it/k0q9frsuir6e1.jpeg\n",
            "[P] I made wut ‚Äì a CLI that explains your last command using a LLM\n",
            "556\n",
            "1hew6wy\n",
            "https://i.redd.it/nwo0a660h17e1.gif\n",
            "[N] AI engineers report burnout and rushed rollouts as ‚Äòrat race‚Äô to stay competitive hits tech industry\n",
            "441\n",
            "1cjdmr5\n",
            "https://www.reddit.com/r/MachineLearning/comments/1cjdmr5/n_ai_engineers_report_burnout_and_rushed_rollouts/\n",
            "[R] Must-Read ML Theory Papers\n",
            "444\n",
            "1gsqqns\n",
            "https://www.reddit.com/r/MachineLearning/comments/1gsqqns/r_mustread_ml_theory_papers/\n",
            "[P] Analysis of why UMAP is so fast \n",
            "434\n",
            "1gsjfq9\n",
            "https://www.reddit.com/r/MachineLearning/comments/1gsjfq9/p_analysis_of_why_umap_is_so_fast/\n",
            "[N] The 2024 Nobel Prize in Chemistry goes to the people Google Deepmind's AlphaFold. One half to David Baker and the other half jointly to Demis Hassabis and John M. Jumper.\n",
            "416\n",
            "1fznxyr\n",
            "https://www.reddit.com/r/MachineLearning/comments/1fznxyr/n_the_2024_nobel_prize_in_chemistry_goes_to_the/\n",
            "[D] I feel like ever since LLM APIs have become a thing the quality of discussion regarding ML and ML products has gone down drastically.\n",
            "416\n",
            "1fl5be0\n",
            "https://www.reddit.com/r/MachineLearning/comments/1fl5be0/d_i_feel_like_ever_since_llm_apis_have_become_a/\n",
            "[D] How did OpenAI go from doing exciting research to a big-tech-like company?\n",
            "399\n",
            "1cvslyc\n",
            "https://www.reddit.com/r/MachineLearning/comments/1cvslyc/d_how_did_openai_go_from_doing_exciting_research/\n",
            "[R] KAN: Kolmogorov-Arnold Networks\n",
            "379\n",
            "1chrafb\n",
            "https://www.reddit.com/r/MachineLearning/comments/1chrafb/r_kan_kolmogorovarnold_networks/\n",
            "You need everything other than ML to win a ML hackathon [D]\n",
            "370\n",
            "1cficmp\n",
            "https://www.reddit.com/r/MachineLearning/comments/1cficmp/you_need_everything_other_than_ml_to_win_a_ml/\n",
            "[P] Drowning in Research Papers? üê∏\n",
            "350\n",
            "1g31hfd\n",
            "https://www.reddit.com/r/MachineLearning/comments/1g31hfd/p_drowning_in_research_papers/\n",
            "[N] Jurgen Schmidhuber on 2024 Physics Nobel Prize\n",
            "358\n",
            "1fzw5b1\n",
            "https://www.reddit.com/r/MachineLearning/comments/1fzw5b1/n_jurgen_schmidhuber_on_2024_physics_nobel_prize/\n",
            "[D] AI Agents: too early, too expensive, too unreliable\n",
            "344\n",
            "1cy1kn9\n",
            "https://www.reddit.com/r/MachineLearning/comments/1cy1kn9/d_ai_agents_too_early_too_expensive_too_unreliable/\n",
            "[N] Sama, an AI sweatshop, pays workers in Kenya $2 an hour to filter and label porn, beastiality, suicide, child abuse, for hours on end!!\n",
            "331\n",
            "1h8nhbh\n",
            "https://youtu.be/qZS50KXjAX0\n",
            "[D] Transformers are a type of CNN\n",
            "331\n",
            "1gaxscv\n",
            "https://www.reddit.com/r/MachineLearning/comments/1gaxscv/d_transformers_are_a_type_of_cnn/\n",
            "[D] Kolmogorov-Arnold Network is just an MLP\n",
            "325\n",
            "1clcu5i\n",
            "https://www.reddit.com/r/MachineLearning/comments/1clcu5i/d_kolmogorovarnold_network_is_just_an_mlp/\n",
            "[P] I made Termite ‚Äì a CLI that can generate terminal UIs from simple text prompts\n",
            "317\n",
            "1hoyzao\n",
            "https://i.redd.it/0gpln74u8t9e1.gif\n",
            "[D] LLMs aren't interesting, anyone else?\n",
            "314\n",
            "1eh4llh\n",
            "https://www.reddit.com/r/MachineLearning/comments/1eh4llh/d_llms_arent_interesting_anyone_else/\n",
            "[P] Illustrated book to learn about Transformers & LLMs\n",
            "313\n",
            "1ew1hws\n",
            "https://www.reddit.com/r/MachineLearning/comments/1ew1hws/p_illustrated_book_to_learn_about_transformers/\n",
            "[D] PyTorch 2.5.0 released!\n",
            "307\n",
            "1g62vyh\n",
            "https://www.reddit.com/r/MachineLearning/comments/1g62vyh/d_pytorch_250_released/\n",
            "[P] Just-in-Time Implementation: A Python Library That Implements Your Code at Runtime\n",
            "302\n",
            "1fujbuz\n",
            "https://www.reddit.com/r/MachineLearning/comments/1fujbuz/p_justintime_implementation_a_python_library_that/\n",
            "[D] Is it common for ML researchers to tweak code until it works and then fit the narrative (and math) around it? \n",
            "292\n",
            "1g40i0h\n",
            "https://www.reddit.com/r/MachineLearning/comments/1g40i0h/d_is_it_common_for_ml_researchers_to_tweak_code/\n",
            "[P]: TensorHue ‚Äì a tensor visualization library (info in comments)\n",
            "287\n",
            "1fbz318\n",
            "https://www.reddit.com/gallery/1fbz318\n",
            "[P] ChessGPT, 100,000x smaller than GPT-4, plays chess at 1500 Elo. By finding a skill vector, we can increase its win rate by 2.6x in out-of-distribution games.\n",
            "284\n",
            "1e8v2za\n",
            "https://www.reddit.com/r/MachineLearning/comments/1e8v2za/p_chessgpt_100000x_smaller_than_gpt4_plays_chess/\n",
            "[P] I made a library for building agents that use tree search to solve problems\n",
            "287\n",
            "1gyreq1\n",
            "https://i.redd.it/qut9unu4su2e1.png\n",
            "[D] Accepted NeurIPS 2024 paper claimed to be solving a novel problem as first work, but ignores 5 prior works\n",
            "282\n",
            "1gxooqv\n",
            "https://www.reddit.com/r/MachineLearning/comments/1gxooqv/d_accepted_neurips_2024_paper_claimed_to_be/\n",
            "[D] OpenAI o3 87.5% High Score on ARC Prize Challenge\n",
            "275\n",
            "1hiq3tz\n",
            "https://www.reddit.com/r/MachineLearning/comments/1hiq3tz/d_openai_o3_875_high_score_on_arc_prize_challenge/\n",
            "[D] Real talk about RAG\n",
            "269\n",
            "1cekoc7\n",
            "https://www.reddit.com/r/MachineLearning/comments/1cekoc7/d_real_talk_about_rag/\n",
            "[D] What are your horror stories from being tasked impossible ML problems\n",
            "269\n",
            "1ccz2cq\n",
            "https://www.reddit.com/r/MachineLearning/comments/1ccz2cq/d_what_are_your_horror_stories_from_being_tasked/\n",
            "[P] I reproduced Anthropic's recent interpretability research \n",
            "268\n",
            "1chsg42\n",
            "https://www.reddit.com/r/MachineLearning/comments/1chsg42/p_i_reproduced_anthropics_recent_interpretability/\n",
            "[D] What‚Äôs the most surprising or counterintuitive insight you‚Äôve learned about machine learning recently?\n",
            "267\n",
            "1gujfj2\n",
            "https://www.reddit.com/r/MachineLearning/comments/1gujfj2/d_whats_the_most_surprising_or_counterintuitive/\n",
            "[N] Ilya Sutskever and friends launch Safe Superintelligence Inc.\n",
            "263\n",
            "1djrs3n\n",
            "https://www.reddit.com/r/MachineLearning/comments/1djrs3n/n_ilya_sutskever_and_friends_launch_safe/\n",
            "[D] - Why MAMBA did not catch on? \n",
            "256\n",
            "1hpg91o\n",
            "https://www.reddit.com/r/MachineLearning/comments/1hpg91o/d_why_mamba_did_not_catch_on/\n",
            "[D] What's the endgame for AI labs that are spending billions on training generative models?\n",
            "250\n",
            "1dsnk1k\n",
            "https://www.reddit.com/r/MachineLearning/comments/1dsnk1k/d_whats_the_endgame_for_ai_labs_that_are_spending/\n",
            "[R] Were RNNs All We Needed?\n",
            "248\n",
            "1fvg7qr\n",
            "https://www.reddit.com/r/MachineLearning/comments/1fvg7qr/r_were_rnns_all_we_needed/\n",
            "[R] Training models with multiple losses\n",
            "244\n",
            "1fbvuhs\n",
            "https://www.reddit.com/r/MachineLearning/comments/1fbvuhs/r_training_models_with_multiple_losses/\n",
            "[N] Llama 3.1 405B launches\n",
            "245\n",
            "1eaaq05\n",
            "https://www.reddit.com/r/MachineLearning/comments/1eaaq05/n_llama_31_405b_launches/\n",
            "[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time\n",
            "\n",
            "242\n",
            "1cqv5y4\n",
            "https://www.reddit.com/r/MachineLearning/comments/1cqv5y4/r_our_new_classification_algorithm_outperforms/\n",
            "[D] What's up with papers without code?\n",
            "237\n",
            "1ct45hk\n",
            "https://www.reddit.com/r/MachineLearning/comments/1ct45hk/d_whats_up_with_papers_without_code/\n",
            "[D] Theory behind modern diffusion models\n",
            "240\n",
            "1h1vxe1\n",
            "https://www.reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/\n",
            "Meta AI (FAIR) latest paper integrates system-1 and system-2 thinking into reasoning models. [R]\n",
            "236\n",
            "1g9v7ag\n",
            "https://www.reddit.com/r/MachineLearning/comments/1g9v7ag/meta_ai_fair_latest_paper_integrates_system1_and/\n",
            "[D] Why do juniors (undergraduates or first- to second-year PhD students) have so many papers at major machine learning conferences like ICML, ICLR, NeurIPS, etc.?\n",
            "236\n",
            "1cidsz7\n",
            "https://www.reddit.com/r/MachineLearning/comments/1cidsz7/d_why_do_juniors_undergraduates_or_first_to/\n",
            "[D] How would you diagnose these spikes in the training loss? \n",
            "228\n",
            "1cf4gw9\n",
            "https://i.redd.it/bk7wbahyh7xc1.png\n",
            "[R] Differential Transformer\n",
            "233\n",
            "1g13gkd\n",
            "https://www.reddit.com/gallery/1g13gkd\n",
            "[D] Is there an alternative to Science Twitter/X?\n",
            "228\n",
            "1gikys5\n",
            "https://www.reddit.com/r/MachineLearning/comments/1gikys5/d_is_there_an_alternative_to_science_twitterx/\n",
            "[D] How do researchers in hot topics keep up?\n",
            "223\n",
            "1flz1vo\n",
            "https://www.reddit.com/r/MachineLearning/comments/1flz1vo/d_how_do_researchers_in_hot_topics_keep_up/\n",
            "[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?\n",
            "215\n",
            "1cq3uh4\n",
            "https://www.reddit.com/r/MachineLearning/comments/1cq3uh4/d_impact_of_solar_storm_on_qlora_rlhf_of_llama3_8b/\n",
            "[D] i sensed anxiety and frustration at NeurIPS‚Äô24 (kyunghyuncho blog)\n",
            "211\n",
            "1hjp5gc\n",
            "https://kyunghyuncho.me/i-sensed-anxiety-and-frustration-at-neurips24/\n",
            "[D] What ML Concepts Do People Misunderstand the Most?\n",
            "215\n",
            "1hjiana\n",
            "https://www.reddit.com/r/MachineLearning/comments/1hjiana/d_what_ml_concepts_do_people_misunderstand_the/\n",
            "[D] Why is there so little statistical analyses in ML research?\n",
            "214\n",
            "1fznaa9\n",
            "https://www.reddit.com/r/MachineLearning/comments/1fznaa9/d_why_is_there_so_little_statistical_analyses_in/\n",
            "[R] Playable 20FPS Doom via a finetuned SD1.4 model from Google research team\n",
            "212\n",
            "1f31uye\n",
            "https://arxiv.org/abs/2408.14837\n",
            "[D] Any OCR recommendations for illegible handwriting?\n",
            "213\n",
            "1h7x5us\n",
            "https://www.reddit.com/gallery/1h7x5us\n",
            "[D] what is the hardest thing as a machine learning engineer\n",
            "211\n",
            "1eifnvj\n",
            "https://www.reddit.com/r/MachineLearning/comments/1eifnvj/d_what_is_the_hardest_thing_as_a_machine_learning/\n",
            "[P] mamba.np: pure NumPy implementation of Mamba\n",
            "210\n",
            "1d80t26\n",
            "https://www.reddit.com/r/MachineLearning/comments/1d80t26/p_mambanp_pure_numpy_implementation_of_mamba/\n",
            "[N] GPT-4o\n",
            "207\n",
            "1cr5lv8\n",
            "https://www.reddit.com/r/MachineLearning/comments/1cr5lv8/n_gpt4o/\n",
            "[R] Meta releases SOTA video generation and audio generation that's less than 40 billion parameters.\n",
            "210\n",
            "1fwic4m\n",
            "https://www.reddit.com/r/MachineLearning/comments/1fwic4m/r_meta_releases_sota_video_generation_and_audio/\n",
            "[D] Coworkers recently told me that the people who think \"LLMs are capable of thinking/understanding\" are the ones who started their ML/NLP career with LLMs. Curious on your thoughts.\n",
            "207\n",
            "1drd3tv\n",
            "https://www.reddit.com/r/MachineLearning/comments/1drd3tv/d_coworkers_recently_told_me_that_the_people_who/\n",
            "[D] Best survey papers of 2024?\n",
            "202\n",
            "1hgwjqu\n",
            "https://www.reddit.com/r/MachineLearning/comments/1hgwjqu/d_best_survey_papers_of_2024/\n",
            "[D] Why does it seem like Google's TPU isn't a threat to nVidia's GPU?\n",
            "212\n",
            "1g1okem\n",
            "https://www.reddit.com/r/MachineLearning/comments/1g1okem/d_why_does_it_seem_like_googles_tpu_isnt_a_threat/\n",
            "[R] Differential Transformer (Microsoft Research)\n",
            "199\n",
            "1fz0pya\n",
            "https://arxiv.org/abs/2410.05258\n",
            "[D] NeurIPS 2024 Paper Reviews\n",
            "201\n",
            "1efscr2\n",
            "https://www.reddit.com/r/MachineLearning/comments/1efscr2/d_neurips_2024_paper_reviews/\n",
            "[P] I was struggle how Stable Diffusion works, so I decided to write my own from scratch with math explanation ü§ñ\n",
            "197\n",
            "1e1w2rg\n",
            "https://www.reddit.com/gallery/1e1w2rg\n",
            "[D] What's your All-Time Favorite Deep Learning Paper?\n",
            "196\n",
            "1d3lbep\n",
            "https://www.reddit.com/r/MachineLearning/comments/1d3lbep/d_whats_your_alltime_favorite_deep_learning_paper/\n",
            "[D] What‚Äôs a machine learning paper or research breakthrough from the last year that everyone should know about?\n",
            "197\n",
            "1gujge8\n",
            "https://www.reddit.com/r/MachineLearning/comments/1gujge8/d_whats_a_machine_learning_paper_or_research/\n",
            "[D] Why ML PhD is so competitive?\n",
            "196\n",
            "1gu9os9\n",
            "https://www.reddit.com/r/MachineLearning/comments/1gu9os9/d_why_ml_phd_is_so_competitive/\n",
            "[D] LLMs: Why does in-context learning work? What exactly is happening from a technical perspective?\n",
            "200\n",
            "1cdih0a\n",
            "https://www.reddit.com/r/MachineLearning/comments/1cdih0a/d_llms_why_does_incontext_learning_work_what/\n",
            "[D] OpenAI new reasoning model called o1\n",
            "196\n",
            "1ff8f7v\n",
            "https://www.reddit.com/r/MachineLearning/comments/1ff8f7v/d_openai_new_reasoning_model_called_o1/\n",
            "[R] How Google Overcame Training Data Issues For Medical AI\n",
            "187\n",
            "1gb7twh\n",
            "https://www.reddit.com/r/MachineLearning/comments/1gb7twh/r_how_google_overcame_training_data_issues_for/\n",
            "[D] Is the new norm for NLP papers \"prompt engineering\" papers?\n",
            "186\n",
            "1ei9e3l\n",
            "https://www.reddit.com/r/MachineLearning/comments/1ei9e3l/d_is_the_new_norm_for_nlp_papers_prompt/\n",
            "[D] What are the (un)written rules of deep learning training \n",
            "186\n",
            "1he07vr\n",
            "https://www.reddit.com/r/MachineLearning/comments/1he07vr/d_what_are_the_unwritten_rules_of_deep_learning/\n",
            "[R] Dynamic Attention-Guided Diffusion for Image Super-Resolution\n",
            "186\n",
            "1gekbg3\n",
            "https://www.reddit.com/r/MachineLearning/comments/1gekbg3/r_dynamic_attentionguided_diffusion_for_image/\n",
            "[D] Has ML actually moved the needle on human health?\n",
            "178\n",
            "1cwsbyw\n",
            "https://www.reddit.com/r/MachineLearning/comments/1cwsbyw/d_has_ml_actually_moved_the_needle_on_human_health/\n",
            "Built gpt2 in C [P]\n",
            "175\n",
            "1fhjtyo\n",
            "https://www.reddit.com/r/MachineLearning/comments/1fhjtyo/built_gpt2_in_c_p/\n",
            "[D] \"Grok\" means way too many different things\n",
            "177\n",
            "1dqpyb3\n",
            "https://www.reddit.com/r/MachineLearning/comments/1dqpyb3/d_grok_means_way_too_many_different_things/\n",
            "[Research] xLSTM: Extended Long Short-Term Memory\n",
            "177\n",
            "1cmwljs\n",
            "https://www.reddit.com/r/MachineLearning/comments/1cmwljs/research_xlstm_extended_long_shortterm_memory/\n",
            "[P] Curated list of LLM papers 2024\n",
            "175\n",
            "1he4htl\n",
            "https://magazine.sebastianraschka.com/p/llm-research-papers-the-2024-list\n",
            "[D] AMA: I‚Äôm Head of AI at a firm in the UK, advising Gov., industry, etc. \n",
            "177\n",
            "1gq899s\n",
            "https://www.reddit.com/r/MachineLearning/comments/1gq899s/d_ama_im_head_of_ai_at_a_firm_in_the_uk_advising/\n",
            "[R] What is your Recipe for Training Neural Networks in 2024?\n",
            "172\n",
            "1giovxi\n",
            "https://www.reddit.com/r/MachineLearning/comments/1giovxi/r_what_is_your_recipe_for_training_neural/\n",
            "[D] Isn't hallucination a much more important study than safety for LLMs at the current stage?\n",
            "173\n",
            "1d329nt\n",
            "https://www.reddit.com/r/MachineLearning/comments/1d329nt/d_isnt_hallucination_a_much_more_important_study/\n",
            "[R] Are you a reviewer for NeurIPS'24? Please read this\n",
            "173\n",
            "1d9o8tn\n",
            "https://www.reddit.com/r/MachineLearning/comments/1d9o8tn/r_are_you_a_reviewer_for_neurips24_please_read/\n",
            "[D] Modern best coding practices for Pytorch (for research)?\n",
            "175\n",
            "1chxpka\n",
            "https://www.reddit.com/r/MachineLearning/comments/1chxpka/d_modern_best_coding_practices_for_pytorch_for/\n",
            "[R] I got my first publication!\n",
            "169\n",
            "1f1ove1\n",
            "https://www.reddit.com/r/MachineLearning/comments/1f1ove1/r_i_got_my_first_publication/\n",
            "[D] What makes a good PhD student in ML\n",
            "171\n",
            "1gplmzb\n",
            "https://www.reddit.com/r/MachineLearning/comments/1gplmzb/d_what_makes_a_good_phd_student_in_ml/\n",
            "[D] What makes TikTok's recommendation algorithm so strong?\n",
            "176\n",
            "1hcp4xw\n",
            "https://www.reddit.com/r/MachineLearning/comments/1hcp4xw/d_what_makes_tiktoks_recommendation_algorithm_so/\n",
            "[D] Please consider signing this letter to open source AlphaFold3\n",
            "167\n",
            "1cqndld\n",
            "https://www.reddit.com/r/MachineLearning/comments/1cqndld/d_please_consider_signing_this_letter_to_open/\n",
            "[D] Reviewers you all need to stop being so lazy dog. Why are reviewers doing things so lazy man? \n",
            "163\n",
            "1cnn54n\n",
            "https://www.reddit.com/r/MachineLearning/comments/1cnn54n/d_reviewers_you_all_need_to_stop_being_so_lazy/\n",
            "[R] Waving Goodbye to Low-Res: A Diffusion-Wavelet Approach for Image Super-Resolution\n",
            "160\n",
            "1eo2xs9\n",
            "https://www.reddit.com/r/MachineLearning/comments/1eo2xs9/r_waving_goodbye_to_lowres_a_diffusionwavelet/\n",
            "[D] What are issues in AI/ML that no one seems to talk about?\n",
            "164\n",
            "1dup0vs\n",
            "https://www.reddit.com/r/MachineLearning/comments/1dup0vs/d_what_are_issues_in_aiml_that_no_one_seems_to/\n",
            "[D] The Parallelism Tradeoff: Understanding Transformer Expressivity Through Circuit Complexity\n",
            "161\n",
            "1hnnl6s\n",
            "https://www.reddit.com/r/MachineLearning/comments/1hnnl6s/d_the_parallelism_tradeoff_understanding/\n",
            "[D] What industry has the worst data?\n",
            "158\n",
            "1eyj7vq\n",
            "https://www.reddit.com/r/MachineLearning/comments/1eyj7vq/d_what_industry_has_the_worst_data/\n",
            "[R] Dynamic Gaussians Mesh\n",
            "159\n",
            "1cfraht\n",
            "https://i.redd.it/y9r6db5wtcxc1.gif\n",
            "[D] Has torch.compile killed the case for JAX?\n",
            "162\n",
            "1ghw330\n",
            "https://www.reddit.com/r/MachineLearning/comments/1ghw330/d_has_torchcompile_killed_the_case_for_jax/\n",
            "[P] Updates on OpenCL backend for Pytorch\n",
            "163\n",
            "1euamk8\n",
            "https://www.reddit.com/r/MachineLearning/comments/1euamk8/p_updates_on_opencl_backend_for_pytorch/\n",
            "[D] ML Researchers in Industry: How Do You Find Time to Publish Papers?\n",
            "157\n",
            "1divk13\n",
            "https://www.reddit.com/r/MachineLearning/comments/1divk13/d_ml_researchers_in_industry_how_do_you_find_time/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subs_top = list(MLsubreddit.top('year', limit=1000))\n",
        "print(len(subs_top))\n",
        "#max 993"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gSdAgO-GofUn",
        "outputId": "6865ff04-efc5-43bc-d9d6-9997b59766a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2207082960.py:1: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
            "Call this function with 'time_filter' as a keyword argument.\n",
            "  subs_top = list(MLsubreddit.top('year', limit=1000))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subs_hot = list(MLsubreddit.hot(limit=1000))\n",
        "print(len(subs_hot))\n",
        "#max 169\n",
        "\n",
        "for sub in subs_hot:\n",
        "  print(sub.title , \" \",sub.score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dYPhSgOppVjF",
        "outputId": "4ef83458-47b5-487e-f2fc-eee5769fceab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "180\n",
            "[D] Simple Questions Thread   5\n",
            "[D] Self-Promotion Thread   2\n",
            "[D] People in ML/DS/AI field since 5-10 years or more, are you tired of updating yourself with changing tech stack?   37\n",
            "[P] Small and Imbalanced dataset - what to do   18\n",
            "Problem with dataset for my my physics undergraduate paper. Need advice about potential data leakage. [N]   3\n",
            "[R] Code for Flow Stochastic Segmentation Networks (ICCV 20205)   2\n",
            "[P] Can I use test set reviews to help predict ratings, or is that cheating?   2\n",
            "custom Vulkan C++ machine learning library vs TensorFlow [R]   1\n",
            "[D] Best way to partition longitudinal data into pre and post time periods for predictive model?   3\n",
            "[2507.17338] Mobile Manipulation with Active Inference for Long-Horizon Rearrangement Tasks   1\n",
            "[D] Got Spare Time ‚Äì What‚Äôs Worth Doing?   36\n",
            "[P] Sensor calibration correction   7\n",
            "[R] Fuzzy-Pattern Tsetlin Machine   37\n",
            "[D] Google DeepMind Analytics Engineer Interview Prep   16\n",
            "[R] Position: The Current AI Conference Model is Unsustainable!   350\n",
            "[D] Statement on the Originality of OpenRLHF and veRL FSDP RLHF   10\n",
            "[D] If there were to be some sort of way you could get NDVI (not true, but predict) that was near perfect accuracy through JUST standard RGB input (NO NIR AT ALL), how useful would that be (API, for example)?   0\n",
            "[D] Multiple submission policy at EMNLP 2025 for workshops   5\n",
            "[D] EMNLP 2025 Decisions   0\n",
            "[D] Applying Prioritized Experience Replay in the PPO algorithm   1\n",
            "Guidance on improving the reconstruction results of my VAE [Project]   1\n",
            "[P] Dealing with EXTREME class imbalance(0.095% prevalence)   11\n",
            "[N] OpenAI Delivers Gold-medal performance at the 2025 International Olympiad in Informatics   61\n",
            "[R] Promising Research Directions for VLMs in the Medical Domain   0\n",
            "[R] AAAI 2026 Reviewer Assignments?   15\n",
            "[R] gpt-oss is actuall good: a case study on SATA-Bench   8\n",
            "[R] About test set of XGBoost for Time Series Forecasting   1\n",
            "[D] Has anyone tried cross-modal transfer for visual reasoning? This 76% MMMU result surprised me   55\n",
            "[P] Can anyone suggest an open weights AI Humanizer?   0\n",
            "[D] Evaluation Drift and Contamination Mitigation in Foundation Model Assessment   1\n",
            "[D] Reliability Metrics and Failure Taxonomy for Agent Tool-Use Systems   1\n",
            "[P] VulkanIlm: Accelerating Local LLM Inference on Older GPUs Using Vulkan (Non-CUDA) ‚Äî Benchmarks Included   30\n",
            "[R]: Intuition emerges in Maximum Caliber models at criticality   0\n",
            "[D] Reminder that Bill Gates's prophesy came true   3438\n",
            "[D] Which direction is better: from academia to industry, or the other way around?   26\n",
            "[P] From GPT-2 to gpt-oss: Analyzing the Architectural Advances And How They Stack Up Against Qwen3   86\n",
            "DRTP and No-Prop Hybrid in Pure C [R]   0\n",
            "[D] Beyond fine-tuning and prompting for LLMs?   5\n",
            "PhDs who publish - how do you get more out of your time [D]   82\n",
            "[R] Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture   11\n",
            "Validation accuracy for FER+ dataset[P]   1\n",
            "[D] Use-case of distribution analysis of numeric features   0\n",
            "[D] how gpt-oss-20b can load in a GPU with only 16 GB of VRAM?   8\n",
            "Any way to visualise 'Grad-CAM'-like attention for multimodal LLMs (gpt, etc.) [P]   7\n",
            "[D] GPT5 is pretty bad with information extraction tasks   50\n",
            "[P] I used YOLOv12 and Gemini to extract and tag over 100,000 scientific plots.   48\n",
            "[D] How do researchers ACTUALLY write code?   152\n",
            "[D] Are there any papers on using reasoning models in embodied AI?   0\n",
            "[D] What happens if reviewers don't fill out the mandatory acknowledgement in NeurIPS 2025?   14\n",
            "[D] Neurips 2025 being hosted at 3 locations.   60\n",
            "[D] Why is scene edit detection still not at or near 100% accuracy?   0\n",
            "[D] - What AI Engineers do in top companies?   146\n",
            "[D]Help running IDM-VTON (virtual try-on) locally or on Colab ‚Äì hitting memory issues and need alternatives   2\n",
            "[D] open source speech to speech (Voice Agent) model?   0\n",
            "[P] We just open-sourced the first full-stack Deep Research: agent + model + data + training‚Äîreproducible GAIA 82.4   25\n",
            "[R] Adaptive Classifiers: Few-Shot Learning with Continuous Adaptation and Dynamic Class Addition   20\n",
            "[R] A quick question to Mathematica + LLM users   0\n",
            "Managing GPU jobs across CoreWeave/Lambda/RunPod is a mess, so im building a simple dashboard[P]   10\n",
            "[D] What would a measurable test for minimal AI welfare look like?   0\n",
            "[D] Neurips rebuttal score change   25\n",
            "[D] Looking for convex-constrained ML problems for benchmarks   8\n",
            "[D] Disentanglement using Flow matching   18\n",
            "[D] Can LLMs Have Accurate World Models?   41\n",
            "[P] Explaining GNN Predictions on \"\"linear\"\" DFGs - GNN experts I need your help <3   0\n",
            "[R] CRINN: Free & Fast Framework for Approximate Nearest Neighbors Search   15\n",
            "[D] In 2025, what is a sufficient methodology to analyze document summaries generated by LLMs? BERTScore, G-Eval, Rogue, etc   9\n",
            "[D]papers on graph neural networks   0\n",
            "[R] Live coding benchmark: GPT-5, Claude Sonnet 4, Gemini 2.5 Pro, GLM45 ‚Äî same prompt, varying difficulty   0\n",
            "[D] Have any Bayesian deep learning methods achieved SOTA performance in...anything?   91\n",
            "[D] Looking for ideas for a ML initiative   0\n",
            "[D] Unsaturated Evals before GPT5   19\n",
            "[P] Reproducing YOLOv1 From Scratch in PyTorch - Learning to Implement Object Detection from the Original Paper   10\n",
            "[D] Training Whisper Tiny   7\n",
            "[D] GSPO: Qwen3‚Äôs sequence-level RLHF method vs. GRPO - stability & scaling analysis   71\n",
            "[D] Idea for an efficient text diffusion model with adaptive, token-level steps   2\n",
            "[D] LSTMs vs Transformers (Model Selection and Thoughts)   0\n",
            "[R] LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models   21\n",
            "[D] FP4 training methods (request for paper recommendations)   6\n",
            "[D] Is modern academic published zero-sum?   156\n",
            "[D] Do you think LLM memory will ever be solved without fine‚Äëtuning?   14\n",
            "DeepMind Genie3 architecture speculation   144\n",
            "[R] Trainable Dynamic Mask Sparse Attention   4\n",
            "[D] NeurIPS 2025 reviewer Confidential Comment   24\n",
            "[P] From Business Processes to GNN for Next Activity Prediction   3\n",
            "[D] Seeking advice on choosing PhD topic/area   13\n",
            "[D]Improving Hybrid KNN + Keyword Matching Retrieval in OpenSearch (Hit-or-Miss Results)   7\n",
            "[N] Machine Learning Reproducibility Challenge (MLRC) 2025 happening this month at Princeton University   32\n",
            "[D] AAAI 2026 desk reject   6\n",
            "[D] NeurIPS 2025 Final Scores   44\n",
            "[P] sklearn-migrator ‚Äì A library to migrate scikit-learn models across versions   9\n",
            "[P] DocStrange - Open Source Document Data Extractor with free cloud processing for 10k docs/month   50\n",
            "[R] CIKM 2025 Decision   15\n",
            "[D] Is AMD Still a Bad Choice for AI Workloads?   10\n",
            "[P] Implementing Einsum   44\n",
            "[D] What‚Äôs the realistic future of Spiking Neural Networks (SNNs)? Curious to hear your thoughts   55\n",
            "[R] Integrative approach for early detection of Parkinson‚Äôs disease and atypical Parkinsonian syndromes leveraging hemodynamic parameters, motion data & advanced AI models   5\n",
            "[D] ZRIA architecture and P-FAF are baseless   2\n",
            "[D] A not-too-expensive cpu server provider for a month ?   3\n",
            "[D] Strange label studio behavior   0\n",
            "[R] From Taylor Series to Fourier Synthesis: The Periodic Linear Unit   223\n",
            "[D] Is there any AI startups in Germanyüá©üá™ investing time and money in building and training foundational models or working for General Intelligence ?other than Aleph Alpha?   53\n",
            "[R] Kimi K2: Open Agentic Intelligence (Technical Report)   14\n",
            "Building for the era of experience [D]   0\n",
            "[P] Implemented the research paper ‚ÄúMemorizing Transformers‚Äù from scratch with my own additional modifications in architecture and customized training pipeline .   25\n",
            "[D] What happens if none of the reviewers respond for all of the NeurIPS discussion?   19\n",
            "[D] Implementing GPU snapshotting to cut cold starts for large models by 12x   45\n",
            "[R] I‚Äôve read the ASI‚ÄëArch paper ‚Äî AI discovered 106 novel neural architectures. What do you think?   72\n",
            "[D]pi0 used in simulation   1\n",
            "[D] Submitted to KDD for the first time! Can I now upload a preprint to arXiv?   0\n",
            "[D] Looking for help: Need to design arithmetic-economics prompts that humans can solve but AI models fail at   0\n",
            "[D] The AAAI website is Awful and organization feels clumsy :/   61\n",
            "[D] NeurIPS 2025 rebuttals.   79\n",
            "[D] Database selection out of several dozens conflicting schemas for a larger NL2SQL pipeline   3\n",
            "[P] Tri-70B-preview-SFT: Open 70B Parameter LLM for Alignment Research (No RLHF) | Trillion Labs   16\n",
            "[D] Weight Tying in LLM Seems to Force the Last MLP to Become the True Unembedding   19\n",
            "[D] Scientific ML: practically relevant OR only an academic exploration?   58\n",
            "[D] How are hybrid reasoning models trained?   5\n",
            "[D] How to find colloborators to grow a small result?   8\n",
            "[D] How to fairly compare AI training methods when they produce different population sizes?   6\n",
            "[R] Seeking Publicly Available Paired MRI + Genomic/Structured Data for Multimodal ML (Human/Animal/Plant)   2\n",
            "[R] Deepmind's AlphaEarth Foundations helps map our planet in unprecedented detail   95\n",
            "[R] Need Urgent Help Regarding ICCV Submission   6\n",
            "[R] AAAI code appendix   6\n",
            "[D] Monthly Who's Hiring and Who wants to be Hired?   4\n",
            "[P] FOMO(Faster Objects, More Objects)   3\n",
            "[R] How LLMs Are Transforming Recommender Systems ‚Äî New Paper   0\n",
            "[D] Math book recommendations for NN theory   63\n",
            "[R] Has anyone experimented with using Euclidean distance as a probability function instead of cosine distance?   3\n",
            "[P] A Black Box LLM Explainability Metric   1\n",
            "[P] Fine-tuning a fast, local ‚Äútab tab‚Äù code completion model for Marimo notebooks   11\n",
            "[D] New recent and applied ideas for representation learning? (i.g. Matryoshka, Constrastive learning, etc.)   37\n",
            "[D] Is there a method as general as MCTS for imperfect information games?   1\n",
            "[D] First research project ‚Äì feedback on \"Ano\", a new optimizer designed for noisy deep RL (also looking for arXiv endorsement)   32\n",
            "[R] Are AUC/ROC curves \"black box\" metrics?   2\n",
            "[R] Multi-View Contrastive Learning: Principled Framework for 3+ Views and Modalities   7\n",
            "[D] AAAI-2026 Code Submission   9\n",
            "[P] BluffMind: Pure LLM powered card game w/ TTS and live dashboard   26\n",
            "[P] Stand-alone implementation of DeepSeek's Native Sparse Attention in PyTorch   6\n",
            "[R] Introducing SNAC-DB: A New Open-Source Resource for Antibody & NANOBODY¬Æ VHH‚ÄìAntigen Modeling   3\n",
            "[D] Shifting Research Directions: Which Deep Learning Domains Will Be Most Impactful in the Next 5‚Äì6 Years?   33\n",
            "[P] Keyword and Phrase Embedding for Query Expansion   2\n",
            "[2507.19457] GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning   41\n",
            "[P] QLora with HuggingFace Model   1\n",
            "[R] Misuse of ML for a cortical pain biomarker?   7\n",
            "State of the Art SISR [R]   7\n",
            "[P]: `ambient-utils`: A small python package for training diffusion models with \"bad data\".   4\n",
            "[D] AAAI: Not able to update authors   7\n",
            "[D] EMNLP 2025 Track Selection   0\n",
            "[P] Built a modern cookiecutter for ML projects - Lets make it better   0\n",
            "[P] I tried implementing the CRISP paper from Google Deepmind in Python   72\n",
            "[D] Pattern recognition is not intelligence, just an important part of the structure   0\n",
            "[P] AI Learns to Play Metal Slug (Deep Reinforcement Learning) With Stable-R...   14\n",
            "[P] Reinforcement Learning from Human Feedback (RLHF) in Notebooks   9\n",
            "[R] Sapient Hierarchical Reasoning Model. HRM.   0\n",
            "[P] Sub-millisecond GPU Task Queue: Optimized CUDA Kernels for Small-Batch ML Inference on GTX 1650.   65\n",
            "[P] LLM Economist: Large Population Models and Mechanism Design via Multi‚ÄëAgent Language Simulacra   14\n",
            "[D] Why CDF normalization is not used in ML? Leads to more uniform distributions - better for generalization   108\n",
            "[P] AI-Failsafe-Overlay ‚Äì Formal alignment recovery framework (misalignment gates, audit locks, recursion filters)   0\n",
            "[P] LLM Context Manager   7\n",
            "[D] Do you think that Muon Optimizer can be viewed through the lens of explore-exploit?   21\n",
            "[R] NeurIPS 2025 D&B: \"The evaluation is limited to 15 open-weights models ... Score: 3\"   328\n",
            "[N] PapersWithCode sunsets, new HuggingFace Papers UI   101\n",
            "[P] Tried Everything, Still Failing at CSLR with Transformer-Based Model   8\n",
            "[R] Training small transformer model on WikiText2 from scratch   2\n",
            "[D] Constructing semantic spaces from given spaces?   2\n",
            "[D] How to improve pretraining pipeline   5\n",
            "[D]: DDPMs: Training learns to undo entire noise, but at sampling time, noise removed step by step, why?   13\n",
            "[P] Build an MLP and Visualize Training in Real Time In Your Browser   4\n",
            "[D] Review Confidence Guidelines   63\n",
            "[D] BMVC 2025 Results Discussion   6\n",
            "[D] Tried of the same review pattern   125\n",
            "[D] Is this Lambda AI rig in demand anymore?   1\n",
            "[P] üöÄBuilt another 124m parameters transformer based model from scratch.This time with multi GPU training with DDP. Inspired from nanoGPT but redesigned to suit my own training pipeline.Model and training code is here   7\n",
            "[D] [MLOps] How to Handle Accuracy Drop in a Few Models During Mass Migration to a New Container?   7\n",
            "[D] AACL VS. AAAI for NLP papers   0\n",
            "[D] How to calculate the memory needed to train your model on GPU   7\n",
            "[D] - NeurIPS'2025 D&B Track   23\n",
            "[R] Question about the NeurIPS 2025 rebuttal process   5\n",
            "Help Needed: Accurate Offline Table Extraction from Scanned Forms [P]   3\n",
            "[D] ACL ARR July 2025 Discussion   12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "subs_controversial= list(MLsubreddit.controversial(limit=1000))\n",
        "print(len(subs_controversial))\n",
        "#max 996"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Rs1ZtbCkv2rw",
        "outputId": "b192c1c5-3814-44f8-909a-1b9f5088c6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('max_colwidth', None)\n",
        "\n",
        "def submissions_to_df(submissions):\n",
        "    col = ['id', 'title', 'author', 'score', 'num_comments', 'created_utc', 'selftext', 'subreddit']\n",
        "    data = []\n",
        "    for sub in submissions:\n",
        "        data.append([\n",
        "            sub.id,\n",
        "            sub.title,\n",
        "            sub.author,\n",
        "            sub.score,\n",
        "            sub.num_comments,\n",
        "            sub.created_utc,\n",
        "            sub.selftext,\n",
        "            sub.subreddit\n",
        "        ])\n",
        "    return pd.DataFrame(data, columns=col)\n",
        "\n",
        "# from submissions to dataframes will be used for analysis, visualizations later\n",
        "df_top_posts= submissions_to_df(subs_top)\n",
        "df_hot_posts = submissions_to_df(subs_hot)\n",
        "df_controversial_posts = submissions_to_df(subs_controversial)\n",
        "\n"
      ],
      "metadata": {
        "id": "iHBIGTr2wST8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding this source col so i can get them back clean to its original state without losing the source\n",
        "df_top_posts['source'] = 't'\n",
        "df_hot_posts['source'] = 'h'\n",
        "df_controversial_posts['source'] = 'c'"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IRhmtNp-wj3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################ took 15 min for execution take care ######################################\n",
        "def get_comments_for_submission(submission):\n",
        "\n",
        "    submission.comment_sort = \"top\"\n",
        "    submission.comment_limit = 10     # only getting the first 10 comments sorting type(hot)\n",
        "    submission.comments.replace_more(limit=0) # dont traverse through the whole comments tree\n",
        "    comments = submission.comments[:10]\n",
        "    return sorted(comments, key=lambda x: x.score, reverse=True)\n",
        "\n",
        "\n",
        "def get_comments_df(submissions):\n",
        "\n",
        "    all_comments = []\n",
        "    for sub in submissions:\n",
        "        comments = get_comments_for_submission(sub)\n",
        "        for c in comments:\n",
        "            all_comments.append([\n",
        "                c.id,\n",
        "                str(c.author),\n",
        "                c.body,\n",
        "                c.score,\n",
        "                c.created_utc,\n",
        "                c.parent_id,\n",
        "                c.link_id\n",
        "            ])\n",
        "    return pd.DataFrame(all_comments, columns=[\n",
        "        'id', 'author', 'body', 'score', 'created_utc', 'parent_id', 'link_id'\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "df_top_comments = get_comments_df(subs_top)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EMct4ffz0eAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hot_comments = get_comments_df(subs_hot)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EK3OPRRk6bN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_controversial_comments = get_comments_df(subs_controversial)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3l64q-qWYPoy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "3919ea9f-4544-4c89-b94c-3acf781a1324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TooManyRequests",
          "evalue": "received 429 HTTP response",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3069174422.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_controversial_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_comments_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubs_controversial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-26231157.py\u001b[0m in \u001b[0;36mget_comments_df\u001b[0;34m(submissions)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mall_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msub\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubmissions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mcomments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_comments_for_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             all_comments.append([\n",
            "\u001b[0;32m/tmp/ipython-input-26231157.py\u001b[0m in \u001b[0;36mget_comments_for_submission\u001b[0;34m(submission)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment_sort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"top\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcomments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/praw/models/reddit/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;34m\"\"\"Return the value of ``attribute``.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{self.__class__.__name__!r} object has no attribute {attribute!r}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/praw/models/reddit/submission.py\u001b[0m in \u001b[0;36m_fetch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m         \u001b[0msubmission_listing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0mcomment_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mListing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomment_listing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/praw/models/reddit/submission.py\u001b[0m in \u001b[0;36m_fetch_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_additional_fetch_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAPI_PATH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/praw/util/deprecate_args.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 )\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_old_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/praw/reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mClientException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m             return self._core.request(\n\u001b[0m\u001b[1;32m    964\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"api_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murljoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moauth_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         return self._request_with_retries(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    265\u001b[0m             )\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATUS_EXCEPTIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATUS_EXCEPTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"no_content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTooManyRequests\u001b[0m: received 429 HTTP response"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adding this so i can combine them back clean\n",
        "df_top_comments['source'] = 't'\n",
        "df_hot_comments['source'] = 'h'\n",
        "df_controversial_comments['source'] = 'c'"
      ],
      "metadata": {
        "id": "vn1cBHFY18z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Some insights\n",
        "\n",
        "### all posts count is 2159\n",
        "### all comments count is 9046"
      ],
      "metadata": {
        "id": "kBiivGCiaGF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenation for cleaning and profiling\n",
        "\n",
        "df_all_posts = pd.concat(\n",
        "    [df_top_posts, df_hot_posts, df_controversial_posts],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "\n",
        "df_all_comments = pd.concat(\n",
        "    [df_top_comments, df_hot_comments, df_controversial_comments],\n",
        "    ignore_index=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "Evyvhs7iApUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_posts.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nTMcfnDSd_OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#using spark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType ,DoubleType\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"title\", StringType(), True),\n",
        "    StructField(\"author\", StringType(), True),\n",
        "    StructField(\"score\", IntegerType(), True),\n",
        "    StructField(\"num_comments\", IntegerType(), True),\n",
        "    StructField(\"created_utc\", DoubleType(), True),\n",
        "    StructField(\"selftext\", StringType(), True),\n",
        "    StructField(\"subreddit\", StringType(), True)\n",
        "])\n",
        "\n",
        "df_spark_all_posts_bronze = spark.createDataFrame(df_all_posts, schema=schema)\n",
        "df_spark_all_posts_bronze.printSchema()'''\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XsbB_rE8Y3EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Profiling And Cleaning"
      ],
      "metadata": {
        "id": "ojeDFG-vicq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_posts_silver =df_all_posts.copy()\n",
        "df_all_posts_silver.columns= [\"post_id\",\"title\",\"author\",\"score\",\"num_comments\",\"created_at\",\"post_content\",\"subreddit\",\"source\"]"
      ],
      "metadata": {
        "id": "Wyp300ZCc5is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_posts_silver.head()"
      ],
      "metadata": {
        "id": "XYV6Ou6yf2T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_posts_silver.describe()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a_CvjxCef5M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df_all_posts_silver.columns:\n",
        "    print(col, df_all_posts_silver[col].dtype)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DIFUHZG4jc9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing obj cols into strings to avoid some potential errors\n",
        "object_cols = df_all_posts_silver.select_dtypes(include='object').columns\n",
        "df_all_posts_silver[object_cols] = df_all_posts_silver[object_cols].astype('string')\n",
        "\n",
        "#changing created_at col type from float into datetime\n",
        "df_all_posts_silver['created_at'] = pd.to_datetime(df_all_posts_silver['created_at'], unit='s').dt.date\n",
        "\n",
        "\n",
        "#df_all_posts_silver['created_at'] = df_all_posts_silver['created_at'].dt.floor('s')\n"
      ],
      "metadata": {
        "id": "tQgdcW4TkaMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df_all_posts_silver.columns:\n",
        "    print(col, df_all_posts_silver[col].dtype)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "86Zq7z1sltj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_posts_silver.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "v0n7_Wx9lutB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_posts_silver.isnull().sum()"
      ],
      "metadata": {
        "id": "KChtHcMvl_fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replacing nulls with Unknown avoidin potential errors\n",
        "df_all_posts_silver['author'] = df_all_posts_silver['author'].fillna('Unknown')\n",
        "df_all_posts_silver.isnull().sum()"
      ],
      "metadata": {
        "id": "J5qFwgZNmLvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_posts_silver.apply(lambda x: x.duplicated().sum())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UoE_Oq22oX77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_posts_silver = df_all_posts_silver.drop_duplicates(subset=['post_id'], keep='first')"
      ],
      "metadata": {
        "id": "-uvcjfDeom-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_posts_silver.apply(lambda x: x.duplicated().sum())\n",
        "#there was 9 post_id dups and 9 created_at dups before 2159 and after 2150"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a8v5gnn6pjmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_posts_silver = df_all_posts_silver[df_all_posts_silver['post_content'].str.strip() != '']\n",
        "#removing any empty posts before 2150 and after 1787\n"
      ],
      "metadata": {
        "id": "80xFnMBLpmLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_posts_silver.describe()"
      ],
      "metadata": {
        "id": "8J6X4Gmbsw2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_posts_silver.apply(lambda x: x.duplicated().sum())\n",
        "#there was 375 post_content dups and became 13 after"
      ],
      "metadata": {
        "id": "mdsZ0HdmugWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#library for detecting english and eliminating other posts\n",
        "!pip install langdetect\n",
        "\n",
        "from langdetect import detect, DetectorFactory\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "def is_english(text):\n",
        "    try:\n",
        "        return detect(text) == 'en'\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "df_all_posts_silver = df_all_posts_silver[df_all_posts_silver['post_content'].apply(is_english)]\n"
      ],
      "metadata": {
        "id": "ky_dO-FSvWf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_posts_silver['title'] = df_all_posts_silver['title'].str.replace(r'^\\[[^\\]]*\\]\\s*', '', regex=True)\n",
        "print(df_all_posts_silver.count())\n",
        "# row count before english filtering 1786 and after 1774\n",
        "\n",
        "df_all_posts_silver.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "K_XX3vQ8w90Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# starting to clean comments table same what we did with posts"
      ],
      "metadata": {
        "id": "amIsRsl1zaBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_comments.head()"
      ],
      "metadata": {
        "id": "RrQ3QnVlxfXn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_comments_silver =df_all_comments.copy()\n",
        "df_all_comments_silver.columns= [\"comment_id\",\"author\",\"comment_content\",\"score\",\"created_at\",\"parent_id\",\"post_id\",\"source\"]"
      ],
      "metadata": {
        "id": "fyPo7N5G3uwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_comments_silver.describe()"
      ],
      "metadata": {
        "id": "JYO3aNkG5MlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df_all_comments_silver.columns:\n",
        "    print(col, df_all_comments_silver[col].dtype)\n"
      ],
      "metadata": {
        "id": "q5TjVrmd4lDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing obj cols into strings\n",
        "object_cols = df_all_comments_silver.select_dtypes(include='object').columns\n",
        "df_all_comments_silver[object_cols] = df_all_comments_silver[object_cols].astype('string')\n",
        "\n",
        "#changing created_at col type from float into datetime\n",
        "df_all_comments_silver['created_at'] = pd.to_datetime(df_all_comments_silver['created_at'], unit='s').dt.date\n",
        "\n",
        "#df_all_comments_silver['created_at'] = df_all_comments_silver['created_at'].dt.floor('s')\n",
        "\n"
      ],
      "metadata": {
        "id": "qXS55dAY5UU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Note: dropping the parent_id col as its the same as the post_id col because i only got the comment not their replies so there is no hiearchy there that's why they are both the same\n",
        "df_all_comments_silver.drop('parent_id', axis=1, inplace=True)\n",
        "\n",
        "for col in df_all_comments_silver.columns:\n",
        "    print(col, df_all_comments_silver[col].dtype)"
      ],
      "metadata": {
        "id": "WlXVXSYw5rhg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_comments_silver.isnull().sum()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xwpj6DLg5fP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_comments_silver.apply(lambda x: x.duplicated().sum())"
      ],
      "metadata": {
        "id": "PAL_z2qn5uOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_comments_silver = df_all_comments_silver.drop_duplicates(subset=['comment_id'], keep='first')\n",
        "df_all_comments_silver.apply(lambda x: x.duplicated().sum())\n",
        "#before 9046 after 9005"
      ],
      "metadata": {
        "id": "ErYLpR577pQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_comments_silver = df_all_comments_silver[df_all_comments_silver['comment_content'].str.strip() != '']\n",
        "#only one comment was deleted"
      ],
      "metadata": {
        "id": "yYgIH2mw57nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_all_comments_silver = df_all_comments_silver[df_all_comments_silver['comment_content'].apply(is_english)]\n",
        "# 703 comment was dropped (assumed it wasn't english because ITS NOT GOOD WITH EMOJIS AND SMALL TEXT)"
      ],
      "metadata": {
        "id": "hveR7QMdB7a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_comments_silver.count()"
      ],
      "metadata": {
        "id": "3z_itwwyCRuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "!wget -O lid.176.ftz https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz"
      ],
      "metadata": {
        "collapsed": true,
        "id": "I4Sy_8rbCeEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lib provided meta that handle text efficiently mostly used for social media\n",
        "import fasttext\n",
        "\n",
        "model = fasttext.load_model(\"lid.176.ftz\")\n",
        "\n",
        "def is_english_fasttext(text):\n",
        "    try:\n",
        "        prediction = model.predict(text, k=1)\n",
        "        language = prediction[0][0].replace(\"__label__\", \"\")\n",
        "        return language == \"en\"\n",
        "    except:\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "AasxPGnvDCdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_english = df_all_posts_silver[df_all_posts_silver['post_content'].apply(is_english_fasttext)]\n",
        "#nothing was dropped all english"
      ],
      "metadata": {
        "id": "f5e4jSBwDMNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_comments_silver.count()"
      ],
      "metadata": {
        "id": "W63PPM-2Dj0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############### separating the data for potential specific analysis #################\n",
        "\n",
        "df_top_posts_cleaned = df_all_posts_silver[df_all_posts_silver['source'] == 't']\n",
        "\n",
        "df_hot_posts_cleaned = df_all_posts_silver[df_all_posts_silver['source'] == 'h']\n",
        "\n",
        "df_controversial_posts_cleaned = df_all_posts_silver[df_all_posts_silver['source'] == 'c']"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-2ZDYtQuDmEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping the source col we don't need it anymore\n",
        "'''\n",
        "df_top_posts_cleaned.drop('source', axis=1, inplace=True)\n",
        "df_hot_posts_cleaned.drop('source', axis=1, inplace=True)\n",
        "df_controversial_posts_cleaned.drop('source', axis=1, inplace=True)\n",
        "'''"
      ],
      "metadata": {
        "id": "t2OaMrVMG8oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_posts_silver.to_parquet(\"all_cleaned_posts.parquet\", index=False)\n",
        "df_all_comments_silver.to_parquet(\"all_cleaned_comments.parquet\", index=False)"
      ],
      "metadata": {
        "id": "6kwa0ha2Ap3k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}